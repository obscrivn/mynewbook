{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.chunk import RegexpParser\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Joe waited for the train. The train was late. Mary and Samantha took the bus. \n",
    "I looked for Mary and Samantha at the bus stations. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Split into sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Joe waited for the train.', 'The train was late.', 'Mary and Samantha took the bus.', 'I looked for Mary and Samantha at the bus stations.']\n"
     ]
    }
   ],
   "source": [
    "token_text = sent_tokenize(text)\n",
    "print(token_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Non-English sentence split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''NLTK ist Open Source Software. Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.  \n",
    "Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK ist Open Source Software.', 'Der Quellcode wird unter den Bedingungen der Apache License Version 2.0 vertrieben.', 'Die Dokumentation wird unter den Bedingungen der Creative Commons-Lizenz Namensnennung - Nicht kommerziell - Keine abgeleiteten Werke 3.0 in den Vereinigten Staaten verteilt.']\n"
     ]
    }
   ],
   "source": [
    "token_text_german = sent_tokenize(text, language=\"german\")\n",
    "print(token_text_german)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Split into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Joe', 'waited', 'for', 'the', 'train', '.', 'The', 'train', 'was', 'late', '.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'stations', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Tokenize - split in sentence and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Joe', 'waited', 'for', 'the', 'train', '.'], ['The', 'train', 'was', 'late', '.'], ['Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.'], ['I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'stations', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokens = [word_tokenize(t) for t in sent_tokenize(text)]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Tokenize Tweeter Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NoSQL', 'introduction', '-', 'w3resource', 'http://bit.ly/1ngHC5F', '#nosql', '#database', '#webdev']\n"
     ]
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet_text = \"NoSQL introduction - w3resource http://bit.ly/1ngHC5F  #nosql #database #webdev\"\n",
    "result = tknzr.tokenize(tweet_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Stopwords - how many stopwords are in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = stopwords.words('french')\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7. Remove and Add stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(list(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove some words from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')) - set(['again', 'once', 'from']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add could, would, should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "custom_list = ['would','could','should']\n",
    "stop_words.extend(custom_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words from your tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [w for w in words if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 4), ('train', 2), ('Mary', 2), ('Samantha', 2)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = collections.Counter(filtered_words)\n",
    "word_counts.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8. Lemmatize and Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['joe', 'wait', 'for', 'the', 'train', '.', 'the', 'train', 'wa', 'late', '.', 'mari', 'and', 'samantha', 'took', 'the', 'bu', '.', 'I', 'look', 'for', 'mari', 'and', 'samantha', 'at', 'the', 'bu', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "stems = [ps.stem(w) for line in tokens for w in line]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Joe', 'waited', 'for', 'the', 'train', '.', 'The', 'train', 'wa', 'late', '.', 'Mary', 'and', 'Samantha', 'took', 'the', 'bus', '.', 'I', 'looked', 'for', 'Mary', 'and', 'Samantha', 'at', 'the', 'bus', 'station', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [lemmatizer.lemmatize(w) for line in tokens for w in line]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <td>Joe</td>\n",
       "      <td>waited</td>\n",
       "      <td>for</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "      <td>.</td>\n",
       "      <td>The</td>\n",
       "      <td>train</td>\n",
       "      <td>was</td>\n",
       "      <td>late</td>\n",
       "      <td>...</td>\n",
       "      <td>looked</td>\n",
       "      <td>for</td>\n",
       "      <td>Mary</td>\n",
       "      <td>and</td>\n",
       "      <td>Samantha</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>bus</td>\n",
       "      <td>stations</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS tag</th>\n",
       "      <td>NNP</td>\n",
       "      <td>VBD</td>\n",
       "      <td>IN</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>.</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>VBD</td>\n",
       "      <td>JJ</td>\n",
       "      <td>...</td>\n",
       "      <td>VBD</td>\n",
       "      <td>IN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>CC</td>\n",
       "      <td>NNP</td>\n",
       "      <td>IN</td>\n",
       "      <td>DT</td>\n",
       "      <td>NN</td>\n",
       "      <td>NNS</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1    2    3      4  5    6      7    8     9   ...      19  \\\n",
       "Word     Joe  waited  for  the  train  .  The  train  was  late  ...  looked   \n",
       "POS tag  NNP     VBD   IN   DT     NN  .   DT     NN  VBD    JJ  ...     VBD   \n",
       "\n",
       "          20    21   22        23  24   25   26        27 28  \n",
       "Word     for  Mary  and  Samantha  at  the  bus  stations  .  \n",
       "POS tag   IN   NNP   CC       NNP  IN   DT   NN       NNS  .  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_pos_tagged = nltk.pos_tag(words)\n",
    "pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 10. NP-Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Joe/NNP)\n",
      "  waited/VBD\n",
      "  for/IN\n",
      "  (NP the/DT train/NN)\n",
      "  ./.\n",
      "  (NP The/DT train/NN)\n",
      "  was/VBD\n",
      "  late/JJ\n",
      "  ./.\n",
      "  (NP Mary/NNP)\n",
      "  and/CC\n",
      "  (NP Samantha/NNP)\n",
      "  took/VBD\n",
      "  (NP the/DT bus/NN)\n",
      "  ./.\n",
      "  I/PRP\n",
      "  looked/VBD\n",
      "  for/IN\n",
      "  (NP Mary/NNP)\n",
      "  and/CC\n",
      "  (NP Samantha/NNP)\n",
      "  at/IN\n",
      "  (NP the/DT bus/NN)\n",
      "  (NP stations/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(nltk_pos_tagged)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
